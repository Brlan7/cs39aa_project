{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction/Background\n\nThe goal of this model is to predict the next word based on the last word of a given sentence using text generation. Which means that we will need a text document that contains many different sentences to identify patterns. Books are a really good example but, we can't just pick any book. If we were to pick a book from Shakespeare, our model will only learn how Shakespeare spoke, which wouldn't help our case because it won't translate well to modern English. A better pick here would be _The Adventures of Sherlock Holmes._ It is an old book but has been updated recently to modern English. The data has been modified to remove unnecessary text that is irrelevant to us including contents menu, headings, and information about the author. What is left is what we need but still need to do some cleanup such as removing extra spaces. The model will learn to recognize features and characteristics of basic language and then use that understanding to suggest words to complete sentences. \n","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# import all of the python modules/packages you'll need here\nimport pandas as pd\nimport string\nimport re\nimport matplotlib.pyplot as plt\nimport collections","metadata":{"execution":{"iopub.status.busy":"2022-11-02T02:16:23.537690Z","iopub.execute_input":"2022-11-02T02:16:23.538548Z","iopub.status.idle":"2022-11-02T02:16:23.542407Z","shell.execute_reply.started":"2022-11-02T02:16:23.538507Z","shell.execute_reply":"2022-11-02T02:16:23.541786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import dataset and do some cleanup by removing extra spaces. \n","metadata":{}},{"cell_type":"code","source":"file = open(\"../input/sherlock-holmes/Sherlock_Holmes.txt\")\nlines = []\nwords_frequency = dict() \n\nfor i in file:\n    lines.append(i)\n    \n    # This is to count the frequency of every word in the text file. Later we will analyze it further. \n    i = i.strip()\n    i = i.lower()\n    i = i.translate(i.maketrans(\"\", \"\", string.punctuation))\n    words = i.split(\" \")\n    \n    for word in words:\n        if word in words_frequency:\n            words_frequency[word] = words_frequency[word] + 1\n        else:\n            words_frequency[word] = 1\n    \ndata = \"\"\n\nfor i in lines:\n    data = ' '. join(lines)\n    \ndata = data.replace('\\n', '').replace('\\ufeff', '').replace('\"', '')\n# print(data)","metadata":{"execution":{"iopub.status.busy":"2022-11-02T02:26:18.469743Z","iopub.execute_input":"2022-11-02T02:26:18.470159Z","iopub.status.idle":"2022-11-02T02:26:23.976635Z","shell.execute_reply.started":"2022-11-02T02:26:18.470131Z","shell.execute_reply":"2022-11-02T02:26:23.975722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want break down this text file into a list made of different sentences. We will then feed these sentences to our training model so that it learns how to \"write\" English. Each sentence is a token. If we use RegEx to split the data on '.', '?', or '!' it splits into 7279 different sentences. The problem is that there's many sentences that contain only one word. This is because the book contains a lot of dialogue that includes one-word questions. One-word sentences will not be helpful for this project because we won't be able to find a pattern if there isn't another word before or after. This is why splitting the data by just a period '.' will become more useful. If we do this we find that there's 6195 sentences. ","metadata":{}},{"cell_type":"code","source":"sentences = data.split(\".\")\nprint(len(sentences))","metadata":{"execution":{"iopub.status.busy":"2022-11-02T02:16:28.784413Z","iopub.execute_input":"2022-11-02T02:16:28.784699Z","iopub.status.idle":"2022-11-02T02:16:28.791331Z","shell.execute_reply.started":"2022-11-02T02:16:28.784670Z","shell.execute_reply":"2022-11-02T02:16:28.790271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another important piece of information that we care for is how big is a sentence. Well now that we have saved all of our observations (sentences) into an array, we can loop through that array, retrieve the length of each element, and then add it to another array called words. Words will hold the length of each sentence. ","metadata":{}},{"cell_type":"code","source":"words_per_sentence = []\nfor sentence in sentences:\n    words_per_sentence.append(len(sentence.split(\" \")))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-02T02:16:28.792785Z","iopub.execute_input":"2022-11-02T02:16:28.793178Z","iopub.status.idle":"2022-11-02T02:16:28.813944Z","shell.execute_reply.started":"2022-11-02T02:16:28.793150Z","shell.execute_reply":"2022-11-02T02:16:28.812821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To have a better look, a histogram has been created below. It appears that most sentences range from 5-20 words.","metadata":{}},{"cell_type":"code","source":"plt.hist(words_per_sentence, bins = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100])\nplt.title(\"Words per Sentence\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-02T02:48:42.077241Z","iopub.execute_input":"2022-11-02T02:48:42.077589Z","iopub.status.idle":"2022-11-02T02:48:42.243620Z","shell.execute_reply.started":"2022-11-02T02:48:42.077561Z","shell.execute_reply":"2022-11-02T02:48:42.242945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the dictionary \"words_frequency\" that we created at the beginning, we will sort it and get the 10 most common words in the text. With no surprise, the most common words are all stop words. Below you can see their frequency in the bar graph that was created. ","metadata":{}},{"cell_type":"code","source":"top_words = sorted(words_frequency.items(), key=lambda item: item[1], reverse=True)[:10]\nword = []\nfrequency = []\nprint(top_words)\nfor i in top_words:\n    word.append(i[0])\n    frequency.append(i[1])\n        \nplt.bar(word, frequency)\nplt.title(\"Most Common Words\")\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-02T03:03:13.191297Z","iopub.execute_input":"2022-11-02T03:03:13.191582Z","iopub.status.idle":"2022-11-02T03:03:13.326195Z","shell.execute_reply.started":"2022-11-02T03:03:13.191559Z","shell.execute_reply":"2022-11-02T03:03:13.325496Z"},"trusted":true},"execution_count":null,"outputs":[]}]}