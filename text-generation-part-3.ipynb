{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:46.649072Z","iopub.execute_input":"2022-12-04T03:47:46.649493Z","iopub.status.idle":"2022-12-04T03:47:46.654967Z","shell.execute_reply.started":"2022-12-04T03:47:46.649457Z","shell.execute_reply":"2022-12-04T03:47:46.653921Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'distilgpt2'\n\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:46.657396Z","iopub.execute_input":"2022-12-04T03:47:46.658049Z","iopub.status.idle":"2022-12-04T03:47:52.275994Z","shell.execute_reply.started":"2022-12-04T03:47:46.658008Z","shell.execute_reply":"2022-12-04T03:47:52.274855Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\nloading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\nloading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\nloading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\nModel config GPT2Config {\n  \"_name_or_path\": \"distilgpt2\",\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nAdding <|startoftext|> to the vocabulary\nAdding <|pad|> to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\nModel config GPT2Config {\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nloading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\nAll model checkpoint weights were used when initializing GPT2LMHeadModel.\n\nAll the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Embedding(50259, 768)"},"metadata":{}}]},{"cell_type":"code","source":"sentences = pd.read_csv('/kaggle/input/holmes/holmes.csv')['sentence']\nsentences.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:52.277395Z","iopub.execute_input":"2022-12-04T03:47:52.277858Z","iopub.status.idle":"2022-12-04T03:47:52.301931Z","shell.execute_reply.started":"2022-12-04T03:47:52.277821Z","shell.execute_reply":"2022-12-04T03:47:52.301095Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0       ï»¿To Sherlock Holmes she is always _the_ woman.\n1    I have seldom heard him mention her under any ...\n2    In his eyes she eclipses and predominates the ...\n3    It was not that he felt any emotion akin to lo...\n4    All emotions, and that one particularly, were ...\nName: sentence, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"max_length = max([len(tokenizer.encode(sentence)) for sentence in sentences])","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:52.303017Z","iopub.execute_input":"2022-12-04T03:47:52.303362Z","iopub.status.idle":"2022-12-04T03:47:54.129224Z","shell.execute_reply.started":"2022-12-04T03:47:52.303329Z","shell.execute_reply":"2022-12-04T03:47:54.128257Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:54.132285Z","iopub.execute_input":"2022-12-04T03:47:54.132685Z","iopub.status.idle":"2022-12-04T03:47:54.139523Z","shell.execute_reply.started":"2022-12-04T03:47:54.132648Z","shell.execute_reply":"2022-12-04T03:47:54.138487Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"125"},"metadata":{}}]},{"cell_type":"code","source":"class HolmesDataset(Dataset):\n    def __init__(self, txt_list, tokenizer, max_length):\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        for txt in txt_list:\n            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n                                       max_length=max_length, padding=\"max_length\")\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:54.141172Z","iopub.execute_input":"2022-12-04T03:47:54.141963Z","iopub.status.idle":"2022-12-04T03:47:54.150641Z","shell.execute_reply.started":"2022-12-04T03:47:54.141927Z","shell.execute_reply":"2022-12-04T03:47:54.149592Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"dataset = HolmesDataset(sentences, tokenizer, max_length=max_length)\ntrain_size = int(0.9 * len(dataset))\ntrain_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:54.151937Z","iopub.execute_input":"2022-12-04T03:47:54.152981Z","iopub.status.idle":"2022-12-04T03:47:56.142351Z","shell.execute_reply.started":"2022-12-04T03:47:54.152944Z","shell.execute_reply":"2022-12-04T03:47:56.141377Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:56.144124Z","iopub.execute_input":"2022-12-04T03:47:56.144508Z","iopub.status.idle":"2022-12-04T03:47:56.154261Z","shell.execute_reply.started":"2022-12-04T03:47:56.144472Z","shell.execute_reply":"2022-12-04T03:47:56.153195Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(tensor([50257,  1544, 28271,   465, 12450,    13, 50256, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n         50258, 50258, 50258, 50258, 50258]),\n tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]))"},"metadata":{}}]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:56.155569Z","iopub.execute_input":"2022-12-04T03:47:56.156064Z","iopub.status.idle":"2022-12-04T03:47:56.364340Z","shell.execute_reply.started":"2022-12-04T03:47:56.156026Z","shell.execute_reply":"2022-12-04T03:47:56.363071Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"128"},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(output_dir='/Users/brian/Documents/College/NLP', num_train_epochs=1, logging_steps=100, save_steps=5000,\n                                  per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=7e-5,\n                                  warmup_steps=10, weight_decay=0.05, report_to = 'none')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:56.366410Z","iopub.execute_input":"2022-12-04T03:47:56.366905Z","iopub.status.idle":"2022-12-04T03:47:56.375880Z","shell.execute_reply.started":"2022-12-04T03:47:56.366866Z","shell.execute_reply":"2022-12-04T03:47:56.374731Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                              'attention_mask': torch.stack([f[1] for f in data]),\n                                                              'labels': torch.stack([f[0] for f in data])}).train()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:47:56.377736Z","iopub.execute_input":"2022-12-04T03:47:56.378223Z","iopub.status.idle":"2022-12-04T03:51:29.222738Z","shell.execute_reply.started":"2022-12-04T03:47:56.378188Z","shell.execute_reply":"2022-12-04T03:51:29.221748Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 5463\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 1\n  Gradient Accumulation steps = 1\n  Total optimization steps = 5463\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5463' max='5463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5463/5463 03:32, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.960200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.760100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.736800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.802200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.739800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.842700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.695200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.642100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.670500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.674300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.711400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.691400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.777900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.646200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.726300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.689100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.681400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.693500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.650000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.686900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.681800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.654900</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.670200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.711600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.678300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.725600</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.712800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.689900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.640200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.675100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.575000</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.684900</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.627900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.737400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.671600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.691700</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.645400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.630500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.739600</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.761100</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.694200</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.610500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.647700</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.623300</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.651700</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.628200</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.611300</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.733200</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.688800</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.727400</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.708200</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.686100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to /Users/brian/Documents/College/NLP/checkpoint-5000\nConfiguration saved in /Users/brian/Documents/College/NLP/checkpoint-5000/config.json\nModel weights saved in /Users/brian/Documents/College/NLP/checkpoint-5000/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5463, training_loss=0.7113451021136151, metrics={'train_runtime': 212.7394, 'train_samples_per_second': 25.679, 'train_steps_per_second': 25.679, 'total_flos': 174250994688000.0, 'train_loss': 0.7113451021136151, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"/Users/brian/Documents/College/NLP\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:51:29.224497Z","iopub.execute_input":"2022-12-04T03:51:29.224890Z","iopub.status.idle":"2022-12-04T03:51:30.118858Z","shell.execute_reply.started":"2022-12-04T03:51:29.224853Z","shell.execute_reply":"2022-12-04T03:51:30.117919Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Configuration saved in /Users/brian/Documents/College/NLP/config.json\nModel weights saved in /Users/brian/Documents/College/NLP/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"/Users/brian/Documents/College/NLP\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:51:30.122513Z","iopub.execute_input":"2022-12-04T03:51:30.122821Z","iopub.status.idle":"2022-12-04T03:51:31.429528Z","shell.execute_reply.started":"2022-12-04T03:51:30.122793Z","shell.execute_reply":"2022-12-04T03:51:31.428572Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"tokenizer config file saved in /Users/brian/Documents/College/NLP/tokenizer_config.json\nSpecial tokens file saved in /Users/brian/Documents/College/NLP/special_tokens_map.json\nadded tokens file saved in /Users/brian/Documents/College/NLP/added_tokens.json\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"('/Users/brian/Documents/College/NLP/tokenizer_config.json',\n '/Users/brian/Documents/College/NLP/special_tokens_map.json',\n '/Users/brian/Documents/College/NLP/vocab.json',\n '/Users/brian/Documents/College/NLP/merges.txt',\n '/Users/brian/Documents/College/NLP/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"/Users/brian/Documents/College/NLP\")\nmodel = GPT2LMHeadModel.from_pretrained(\"/Users/brian/Documents/College/NLP\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:51:31.430808Z","iopub.execute_input":"2022-12-04T03:51:31.431173Z","iopub.status.idle":"2022-12-04T03:51:32.640108Z","shell.execute_reply.started":"2022-12-04T03:51:31.431138Z","shell.execute_reply":"2022-12-04T03:51:32.639193Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"loading file /Users/brian/Documents/College/NLP/vocab.json\nloading file /Users/brian/Documents/College/NLP/merges.txt\nloading file /Users/brian/Documents/College/NLP/added_tokens.json\nloading file /Users/brian/Documents/College/NLP/special_tokens_map.json\nloading file /Users/brian/Documents/College/NLP/tokenizer_config.json\nAdding <|startoftext|> to the vocabulary\nAdding <|pad|> to the vocabulary\nloading configuration file /Users/brian/Documents/College/NLP/config.json\nModel config GPT2Config {\n  \"_name_or_path\": \"distilgpt2\",\n  \"_num_labels\": 1,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 6,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50259\n}\n\nloading weights file /Users/brian/Documents/College/NLP/pytorch_model.bin\nAll model checkpoint weights were used when initializing GPT2LMHeadModel.\n\nAll the weights of GPT2LMHeadModel were initialized from the model checkpoint at /Users/brian/Documents/College/NLP.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"generated = tokenizer(\"<|startoftext|> He hasn't seen the old man since\", return_tensors=\"pt\").input_ids\nsample_outputs = model.generate(generated, do_sample=True, top_k=45, max_length=40, top_p=0.7, temperature=2.45, num_return_sequences=20)\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T03:53:25.083625Z","iopub.execute_input":"2022-12-04T03:53:25.083992Z","iopub.status.idle":"2022-12-04T03:53:30.468424Z","shell.execute_reply.started":"2022-12-04T03:53:25.083963Z","shell.execute_reply":"2022-12-04T03:53:30.466340Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0:  He hasn't seen the old man since, no better; no better of you when my son walks.\n1:  He hasn't seen the old man since the age of one! He would be as old and as new with a beard and a hat which is a man, when old age dies.\n2:  He hasn't seen the old man since a moment ago.\n3:  He hasn't seen the old man since, with his glasses at his chin.\n4:  He hasn't seen the old man since when his father passed away but now it has gone on forever to give birthâfor if he does come from Winchester and visit her every time of the future\n5:  He hasn't seen the old man since it arrived to him; for all that I had known of him before I saw him when the latter entered, it would appear to me that there are some\n6:  He hasn't seen the old man since I first saw it from where he is?â He is one of seven or eight times the kind of father in this family that goes out all over\n7:  He hasn't seen the old man since.\n8:  He hasn't seen the old man since last morning.\n9:  He hasn't seen the old man since he left the Lodge.\n10:  He hasn't seen the old man since his days.\n11:  He hasn't seen the old man since his young father, and neither has seen him so good at this age of marriage.\n12:  He hasn't seen the old man since he could have seen a few steps down the passage at this instant.\n13:  He hasn't seen the old man since I first came in, no little or less so than an hour now?â I thought he shook his head.\n14:  He hasn't seen the old man since.\n15:  He hasn't seen the old man since? That is my impression which must be borne upon himself.\n16:  He hasn't seen the old man since they broke.\n17:  He hasn't seen the old man since, and we are now in an all time between the words which explain why, with his keen, and his desire to imitate his son; but I shall\n18:  He hasn't seen the old man since then.\n19:  He hasn't seen the old man since he first was out, of all the places with the same people who have become himâor he will rather have been himself, or the woman.\n","output_type":"stream"}]}]}